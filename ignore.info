-- ACTIVATION FUNCTIONS --
relu is good for hidden layers, there is also leaky relu 

tanh is good for hidden layers, but relu prob better for most cases

sigmoid is bad for hidden layers(tanh and relu always better) but good for output layer if binary classification

softmax is good for multi classification

for regression we often dont want any activation function on the output layer (linear function)

An activation function on hidden layer need to be none linear, to make the output have a none linear boundary. 
but we dont wanna squeeze the data too much (vanishing gradient problem) in the hidden layers


-- LOSS FUNCTIONS --
categorical_crossentropy is meant for multi classification where u have multiple output neurons, each neuron represent its probability 0 to 1.0

sparse_categorical_crossentropy is for multiclassification when u have 1 output neuron, where each integer represent 1 classificaiton value

binary_crossentropy is for binary classification, only needs 1 output neuron.

Mean Squared Erroris for regressions, where the output is a continuous value


-- GAN --
GAN is a sequantial network consisting of 2 neural networks, the generator and the discriminator.
The generator start by generating random data, then ask the discriminator for feedback.
The discriminator is just a binary classifier that outputs a float between 0(bad) and 1(good). 
Then the generator use that feeback with gradient descent to slowly improve.
The discriminator needs real image to train and improve but the generator will learn with the help of the discriminator.

