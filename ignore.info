-- ACTIVATION FUNCTIONS --
relu is good for hidden layers, there is also leaky relu 

tanh is good for hidden layers, but relu prob better for most cases

sigmoid is bad for hidden layers(tanh and relu always better) but good for output layer if binary classification

softmax is good for multi classification

for regression we often dont want any activation function on the output layer (linear function)

An activation function on hidden layer need to be none linear, to make the output have a none linear boundary. 
but we dont wanna squeeze the data too much (vanishing gradient problem) in the hidden layers
categorical_crossentropy is meant for multi classification where u have multiple output neurons, each neuron represent its probability 0 to 1.0

sparse_categorical_crossentropy is for multiclassification when u have 1 output neuron, where each integer represent 1 classificaiton value

binary_crossentropy is for binary classification, only needs 1 output neuron.

Mean Squared Erroris for regressions, where the output is a continuous value

