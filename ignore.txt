-- ACTIVATION FUNCTIONS --
The activation function makes it possible for the neural network to predict none-linear relationships in the data.
An activation function on hidden layer need to be none linear, to make the output have a none linear boundary. 
but we dont wanna squeeze the data too much (vanishing gradient problem) in the hidden layers

relu is good for hidden layers, there is also leaky relu 

tanh is good for hidden layers, but relu prob better for most cases

sigmoid is bad for hidden layers(tanh and relu always better) but good for output layer if binary classification

softmax is good for multi classification

Regression we often dont want any activation function on the output layer (linear function)


-- LOSS FUNCTIONS --
A loss function is a function that takes 2 inputs, the expected y_value and the predicted y_value (output of the network).
It will then simply calculate the loss/error of the predicted y_value.

categorical_crossentropy is meant for multi classification where u have multiple output neurons, each neuron represent its probability 0 to 1.0

sparse_categorical_crossentropy is for multiclassification when u have 1 output neuron, where each integer represent 1 classificaiton value

binary_crossentropy is for binary classification, only needs 1 output neuron.

Mean Squared Erroris for regressions, where the output is a continuous value.


-- Optimizer --
A class that updates the parameters/weights of the network.
It has a function called update that takes a list of parameters and the graident (list of values).
The update function iterate over (python zip) lists, and update the parameters in the opposite direction of gradient.
Adam is a common optmizier that uses momentum to avoid getting stuck at flukautions.
Often the optimizer finds the (major)local minimum. 


-- GAN --
GAN is a sequential network consisting of 2 neural networks, the generator and the discriminator.
The generator start by generating random data, then ask the discriminator for feedback.
The discriminator is just a binary classifier that outputs a float between 0(bad) and 1(good). 
Then the generator use that feeback with gradient descent to slowly improve.
The discriminator needs real image to train and improve but the generator will learn with the help of the discriminator.


-- Gradient --
in order to calculate the gradient of a parameter/weight, u just take the partial derivitive of the error functions.

P  = predicted value (output)
A  = Actual value (the correct value)
LR = Learning Rate (hyper parameter)
E  = Error function, in this case mean square error
E  = 0.5(P - A)^2 

Calculate the gradient of weight1, Gw1 
Take the derivitive of E with respect to w1
Gw1 = ∂E/∂w1
Now as E is depending of P which is dependet on w1, we can use the chain rule
--> ∂E/∂w1
--> ∂E/∂P * ∂P/∂w1
--> 2 * 0.5(P-A)^(2-1) * ∂P/∂w1 
--> (P-A) * ∂P/∂w1

Update the weight with SGD:
w1(new) = w1(old) - LR * Gw1




