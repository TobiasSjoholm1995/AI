
relu is good for hidden layers, there is also leaky relu 

tanh is good for hidden layers, but relu prob better for most cases

sigmoid is bad for hidden layers(tanh and relu always better) but good for output layer if binary classification

softmax is good for multi classification

for regression we often dont want any activation function on the output layer (linear function)

An activation function on hidden layer need to be none linear, to make the output have a none linear boundary. 
but we dont wanna squeeze the data too much (vanishing gradient problem) in the hidden layers
